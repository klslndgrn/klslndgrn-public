{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6273326574ec9693a7f67a3679389d2e",
     "grade": false,
     "grade_id": "cell-c374456facd20e46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Testing Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing is one of the most important components of sustainable software development. It improves code quality, maintainability and lifetime, and saves developer time. In previous labs you have already come across this in the form of assert statements. There are a number of testing libraries to support Python development and they all have in common that they build on the assert statement. In addition they provide better information when errors are found and the facilitate automated testing of large projects.\n",
    "\n",
    "We will use the doctest and the pytest libraries. If pytest is not installed you need to install it with pip. On your computers do this in the virtual environment where you previously installed jupyter\n",
    "\n",
    "~~~\n",
    "$ pip install pytest\n",
    "$ jupyter notebook\n",
    "~~~\n",
    "\n",
    "Testing libraries are typically designed to be used on Python source files while they are not adapted to be used with Jupyter notebooks. To be able to work with this in this lab, we use cell magic commands (%%file) to save cells in ordinary files and then execute pytest on those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b419f62e162c6a542eeacf8bdb910bba",
     "grade": false,
     "grade_id": "cell-33440c27e09b63e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import doctest\n",
    "import pytest\n",
    "# This is for jupyter to recognize changes in external files without restarting kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: add time stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d228696c2f97cb4ec99a5ae63c4e9ef",
     "grade": false,
     "grade_id": "cell-0ea01ce4cc34e8ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting timestamps.py\n"
     ]
    }
   ],
   "source": [
    "%%file timestamps.py\n",
    "def sum_timestamps(l):\n",
    "    \"\"\"\n",
    "    >>> sum_timestamps(['5:32', '4:48'])\n",
    "    '10:20'\n",
    "    >>> sum_timestamps(['03:10', '01:00'])\n",
    "    '4:10'\n",
    "    >>> sum_timestamps(['2:10', '1:59'])\n",
    "    '4:09'\n",
    "    >>> sum_timestamps(['15:32', '45:48'])\n",
    "    '1:01:20'\n",
    "    >>> sum_timestamps(['6:15:32', '2:45:48'])\n",
    "    '9:01:20'\n",
    "    >>> sum_timestamps(['6:35:32', '2:45:48', '40:10'])\n",
    "    '10:01:30'\n",
    "    \"\"\"\n",
    "    import datetime\n",
    " \n",
    "    tmstps = datetime.timedelta()\n",
    "    for i in l:\n",
    "        n = i.count(':')\n",
    "        if n == 1:\n",
    "            (m, s) = i.split(':')\n",
    "            t = datetime.timedelta(minutes=int(m), seconds=int(s))\n",
    "            tmstps += t\n",
    "        elif n == 2:\n",
    "            (h, m, s) = i.split(':')\n",
    "            t = datetime.timedelta(hours=int(h), minutes=int(m), seconds=int(s))\n",
    "            tmstps += t\n",
    "    (h,m,s) = str(tmstps).split(':')\n",
    "    if int(h) == 0:\n",
    "        tmstps = [str(int(m)),str(s)]\n",
    "        tmstps = ':'.join(tmstps)\n",
    "    else:\n",
    "        tmstps = [str(h),str(m),str(s)]\n",
    "        tmstps = ':'.join(tmstps)\n",
    "    \n",
    "    return(tmstps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a869f4cd39aaf8cb6b1d3e9635ce59a9",
     "grade": true,
     "grade_id": "cell-210600fd395cba29",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    sum_timestamps(['5:32', '4:48'])\n",
      "Expecting:\n",
      "    '10:20'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['03:10', '01:00'])\n",
      "Expecting:\n",
      "    '4:10'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['2:10', '1:59'])\n",
      "Expecting:\n",
      "    '4:09'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['15:32', '45:48'])\n",
      "Expecting:\n",
      "    '1:01:20'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['6:15:32', '2:45:48'])\n",
      "Expecting:\n",
      "    '9:01:20'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['6:35:32', '2:45:48', '40:10'])\n",
      "Expecting:\n",
      "    '10:01:30'\n",
      "ok\n",
      "1 items had no tests:\n",
      "    timestamps\n",
      "1 items passed all tests:\n",
      "   6 tests in timestamps.sum_timestamps\n",
      "6 tests in 2 items.\n",
      "6 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=6)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timestamps\n",
    "doctest.testmod(timestamps, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: time_stamps with pytest\n",
    "\n",
    "Write a test file to be used with pytest for assignment 1, with the same test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c24f66de4a09c756af120dbc764df2f0",
     "grade": false,
     "grade_id": "cell-ed7cd15078053e50",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_timestamps.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_timestamps.py\n",
    "\n",
    "from timestamps import sum_timestamps\n",
    "\n",
    "def test_sum_time_1():\n",
    "    assert sum_timestamps(['5:32', '4:48']) == '10:20'\n",
    "\n",
    "def test_sum_time_2():\n",
    "    assert sum_timestamps(['03:10', '01:00']) == '4:10'\n",
    "\n",
    "def test_sum_time_3():\n",
    "    assert sum_timestamps(['2:10', '1:59']) == '4:09'\n",
    "\n",
    "def test_sum_time_4():\n",
    "    assert sum_timestamps(['15:32', '45:48']) == '1:01:20'\n",
    "\n",
    "def test_sum_time_5():\n",
    "    assert sum_timestamps(['6:15:32', '2:45:48']) == '9:01:20'\n",
    "\n",
    "def test_sum_time_6():\n",
    "    assert sum_timestamps(['6:35:32', '2:45:48', '40:10']) == '10:01:30'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee29c06d79b8bcedd807baa0cfac3473",
     "grade": true,
     "grade_id": "cell-c597b267d405be1f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.9.7, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- C:\\Users\\klsln\\anaconda3\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\klsln\\Documents\\Python\\BB1000\\A2-Testing\n",
      "plugins: anyio-2.2.0\n",
      "collecting ... collected 6 items\n",
      "\n",
      "test_timestamps.py::test_sum_time_1 PASSED                               [ 16%]\n",
      "test_timestamps.py::test_sum_time_2 PASSED                               [ 33%]\n",
      "test_timestamps.py::test_sum_time_3 PASSED                               [ 50%]\n",
      "test_timestamps.py::test_sum_time_4 PASSED                               [ 66%]\n",
      "test_timestamps.py::test_sum_time_5 PASSED                               [ 83%]\n",
      "test_timestamps.py::test_sum_time_6 PASSED                               [100%]\n",
      "\n",
      "============================== 6 passed in 0.03s ==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest.main(['-v', 'test_timestamps.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aff4a31a576f7a13a7f174e005cf620b",
     "grade": false,
     "grade_id": "cell-a4c4bdbc6f98dac1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 3: running mean\n",
    "\n",
    "This is an example use of parametrized test cases. Look at the test function and understand how it works.\n",
    "Write a function that passes the tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f15c3812d0a4c64c65dfe19947283f0b",
     "grade": false,
     "grade_id": "cell-a62f5af5275891d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_running_mean.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_running_mean.py\n",
    "import pytest\n",
    "\n",
    "from running_mean import running_mean\n",
    "\n",
    "@pytest.mark.parametrize(\"input_argument, expected_return\", [\n",
    "    ([1, 2, 3], [1, 1.5, 2]),\n",
    "    ([2, 6, 10, 8, 11, 10],\n",
    "     [2.0, 4.0, 6.0, 6.5, 7.4, 7.83]),\n",
    "    ([3, 4, 6, 2, 1, 9, 0, 7, 5, 8],\n",
    "     [3.0, 3.5, 4.33, 3.75, 3.2, 4.17, 3.57, 4.0, 4.11, 4.5]),\n",
    "    ([], []),\n",
    "])\n",
    "def test_running_mean(input_argument, expected_return):\n",
    "    ret = list(running_mean(input_argument))\n",
    "    assert ret == expected_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d81563da5f903e237fcfeea5dbe2e314",
     "grade": false,
     "grade_id": "cell-96c10ee58ecac500",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting running_mean.py\n"
     ]
    }
   ],
   "source": [
    "%%file running_mean.py\n",
    "def running_mean(sequence):\n",
    "    \"\"\"Calculate the running mean of the sequence passed in,\n",
    "       returns a sequence of same length with the averages.\n",
    "       You can assume all items in sequence are numeric.\"\"\"\n",
    "\n",
    "    moving_av = []\n",
    "    iter = 0\n",
    "    sum = 0\n",
    "   \n",
    "    for i in sequence:\n",
    "      iter += 1\n",
    "      sum += i\n",
    "      av = round(sum/iter,2)\n",
    "      moving_av.append(av)\n",
    "    return moving_av\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2214c633a7a294cb33de7bbac7d2bad5",
     "grade": true,
     "grade_id": "cell-6a057ac659530fd9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.9.7, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- C:\\Users\\klsln\\anaconda3\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\klsln\\Documents\\Python\\BB1000\\A2-Testing\n",
      "plugins: anyio-2.2.0\n",
      "collecting ... collected 4 items\n",
      "\n",
      "test_running_mean.py::test_running_mean[input_argument0-expected_return0] PASSED [ 25%]\n",
      "test_running_mean.py::test_running_mean[input_argument1-expected_return1] PASSED [ 50%]\n",
      "test_running_mean.py::test_running_mean[input_argument2-expected_return2] PASSED [ 75%]\n",
      "test_running_mean.py::test_running_mean[input_argument3-expected_return3] PASSED [100%]\n",
      "\n",
      "============================== 4 passed in 0.03s ==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest.main(['-v', 'test_running_mean.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "\n",
    "Write a second version of the test function for timestamp where the different test cases are different parameterizations for one test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "878794980360e0e42f6b8eb89986eb3e",
     "grade": false,
     "grade_id": "cell-1d068aaf56835af8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_timestamps_2.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_timestamps_2.py\n",
    "import pytest\n",
    "from timestamps import sum_timestamps\n",
    "\n",
    "@pytest.mark.parametrize(\"test_input,expected\", [\n",
    "    (['5:32', '4:48'], '10:20'),\n",
    "    (['03:10', '01:00'], '4:10'),\n",
    "    (['2:10', '1:59'], '4:09'),\n",
    "    (['15:32', '45:48'], '1:01:20'),\n",
    "    (['6:15:32', '2:45:48'], '9:01:20'),\n",
    "    (['6:35:32', '2:45:48', '40:10'], '10:01:30')])\n",
    "\n",
    "def test_sum_time(test_input, expected):\n",
    "    assert sum_timestamps(test_input) == expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77cf3d292ff9c2ea8171ce5f579ccad6",
     "grade": true,
     "grade_id": "cell-32b906546d801ca5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pytest.main(['-v', 'test_timestamps_2.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAST EXAM QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a test function for the output examples in problem 12., that can be processed by pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 12:\n",
    "\n",
    "    >>> def f(*args):\n",
    "    ... return list(reversed(args))\n",
    "    >>> f(1)\n",
    "    [1]\n",
    "    >>> f(1, 2)\n",
    "    [2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_f():\n",
    "    assert f(1) == [1]\n",
    "    assert f(1, 2) == [2, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
